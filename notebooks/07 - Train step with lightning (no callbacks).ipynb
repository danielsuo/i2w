{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db6a05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extension horovod.torch has not been built: /opt/conda/lib/python3.7/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-37m-x86_64-linux-gnu.so not found\n",
      "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "Warning! MPI libs are missing, but python applications are still avaiable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import functools\n",
    "import inspect\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import flax\n",
    "from flax import jax_utils\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "import init2winit\n",
    "import fastmri\n",
    "\n",
    "import i2w\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import strategies\n",
    "from pytorch_lightning.trainer.states import RunningStage, TrainerFn, TrainerState, TrainerStatus\n",
    "\n",
    "from fastmri.models import unet as t_unet\n",
    "from fastmri.pl_modules import data_module\n",
    "from fastmri.pl_modules import unet_module\n",
    "from fastmri.data.transforms import UnetDataTransform\n",
    "from fastmri.data.subsample import create_mask_for_mask_type\n",
    "from fastmri_examples.unet import train_unet_demo\n",
    "\n",
    "from init2winit.model_lib import unet as f_unet\n",
    "from init2winit.dataset_lib import fastmri_dataset\n",
    "from init2winit.dataset_lib import data_utils\n",
    "from init2winit.optimizer_lib import optimizers\n",
    "from init2winit.optimizer_lib import transform\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0763a57",
   "metadata": {},
   "source": [
    "# Train step with lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac608a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbed from `train_unet_demo.build_args()`.\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    accelerations=[4],\n",
    "    accelerator='gpu',  # Should be `ddp`, but not available in interactive mode\n",
    "    accumulate_grad_batches=None,\n",
    "    amp_backend='native',\n",
    "    amp_level=None,\n",
    "    auto_lr_find=False,\n",
    "    auto_scale_batch_size=False,\n",
    "    auto_select_gpus=False,\n",
    "    batch_size=8,\n",
    "    benchmark=False,\n",
    "#     callbacks=[<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f409795e090>],\n",
    "    center_fractions=[0.08],\n",
    "    challenge='singlecoil',\n",
    "    chans=32,\n",
    "    check_val_every_n_epoch=1,\n",
    "    checkpoint_callback=None,\n",
    "    combine_train_val=False,\n",
    "    data_path=pathlib.PosixPath('/home/dsuo'),\n",
    "    default_root_dir=pathlib.PosixPath('unet/unet_demo'),\n",
    "    detect_anomaly=False, deterministic=True,\n",
    "    devices=None, drop_prob=0.0,\n",
    "    enable_checkpointing=True,\n",
    "    enable_model_summary=True,\n",
    "    enable_progress_bar=True,\n",
    "    fast_dev_run=False,\n",
    "    flush_logs_every_n_steps=None,\n",
    "    gpus=8,\n",
    "    gradient_clip_algorithm=None,\n",
    "    gradient_clip_val=None,\n",
    "    in_chans=1,\n",
    "    ipus=None,\n",
    "    limit_predict_batches=1.0,\n",
    "    limit_test_batches=1.0,\n",
    "    limit_train_batches=1.0,\n",
    "    limit_val_batches=1.0,\n",
    "    log_every_n_steps=50,\n",
    "    log_gpu_memory=None,\n",
    "    logger=True,\n",
    "    lr=0.001,\n",
    "    lr_gamma=0.1,\n",
    "    lr_step_size=40,\n",
    "    mask_type=None,  # Should be `random`, but tying out without\n",
    "    max_epochs=50,\n",
    "    max_steps=-1,\n",
    "    max_time=None,\n",
    "    min_epochs=None,\n",
    "    min_steps=None,\n",
    "    mode='train',\n",
    "    move_metrics_to_cpu=False,\n",
    "    multiple_trainloader_mode='max_size_cycle',\n",
    "    num_log_images=16,\n",
    "    num_nodes=1,\n",
    "    num_pool_layers=4,\n",
    "    num_processes=1,\n",
    "    num_sanity_val_steps=2,\n",
    "    num_workers=4,\n",
    "    out_chans=1,\n",
    "    overfit_batches=0.0,\n",
    "    plugins=None,\n",
    "    precision=32,\n",
    "    prepare_data_per_node=None,\n",
    "    process_position=0,\n",
    "    profiler=None,\n",
    "    progress_bar_refresh_rate=None,\n",
    "    reload_dataloaders_every_epoch=False,\n",
    "    reload_dataloaders_every_n_epochs=0,\n",
    "    replace_sampler_ddp=False,\n",
    "    resume_from_checkpoint=None,\n",
    "    sample_rate=None,\n",
    "    seed=42,\n",
    "    stochastic_weight_avg=False,\n",
    "    strategy='dp',  # This should be None\n",
    "    sync_batchnorm=False,\n",
    "    terminate_on_nan=None,\n",
    "    test_path=None,\n",
    "    test_sample_rate=None,\n",
    "    test_split='test',\n",
    "    test_volume_sample_rate=None,\n",
    "    tpu_cores=None,\n",
    "    track_grad_norm=-1,\n",
    "    use_dataset_cache_file=True,\n",
    "    val_check_interval=1.0,\n",
    "    val_sample_rate=None,\n",
    "    val_volume_sample_rate=None,\n",
    "    volume_sample_rate=None,\n",
    "    weight_decay=0.0,\n",
    "    weights_save_path=None,\n",
    "    weights_summary='top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e456ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = UnetDataTransform(args.challenge, mask_func=args.mask_type, use_seed=False)\n",
    "val_transform = UnetDataTransform(args.challenge, mask_func=args.mask_type)\n",
    "test_transform = UnetDataTransform(args.challenge)\n",
    "\n",
    "dm = data_module.FastMriDataModule(\n",
    "        data_path=args.data_path,\n",
    "        challenge=args.challenge,\n",
    "        train_transform=train_transform,\n",
    "        val_transform=val_transform,\n",
    "        test_transform=test_transform,\n",
    "        test_split=args.test_split,\n",
    "        test_path=args.test_path,\n",
    "        sample_rate=args.sample_rate,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        distributed_sampler=(args.accelerator in (\"ddp\", \"ddp_cpu\")),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065fd19c",
   "metadata": {},
   "source": [
    "## `fastmri`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a74ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model = unet_module.UnetModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b92144",
   "metadata": {},
   "source": [
    "## `init2winit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f542a35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abs sum diff: 0.16495502\n"
     ]
    }
   ],
   "source": [
    "f_model = f_unet.UNetModel(f_unet.DEFAULT_HPARAMS, {}, 'mean_absolute_error', 'image_reconstruction_metrics')\n",
    "f_params = i2w.convert_params(t_model.cpu().unet, f_model.flax_module)\n",
    "\n",
    "f_opt_init, f_opt_update = f_opt = optax.chain(transform.precondition_by_rms(decay=0.99), optax.scale_by_schedule(optax.piecewise_constant_schedule(init_value=-args.lr, boundaries_and_scales={args.lr_step_size: args.lr_gamma})))\n",
    "f_opt_state = f_opt_init(f_params)\n",
    "\n",
    "def loss(params, image, y):\n",
    "    return jnp.abs(f_model.flax_module.apply(params, image) - y).mean()\n",
    "\n",
    "def loop(t_batch, f_params, f_opt_state):\n",
    "    f_batch = i2w.create_batch(t_batch)\n",
    "    f_loss, grad = jax.value_and_grad(loss)(f_params, f_batch['inputs'].squeeze(), f_batch['targets'].squeeze())\n",
    "    updates, f_opt_state = f_opt_update(grad, f_opt_state, f_params)\n",
    "    f_params = optax.apply_updates(f_params, updates)\n",
    "    f_out = f_model.flax_module.apply(f_params, f_batch['inputs'].squeeze())\n",
    "    \n",
    "    return f_loss, f_out, f_params, f_opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2c93f",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef9571",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:464: UserWarning: more than one device specific flag has been set\n",
      "  rank_zero_warn(\"more than one device specific flag has been set\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_predict_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68105a680e0b41b5a4ff58d55150f3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/jax/_src/tree_util.py:189: FutureWarning: jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() instead as a drop-in replacement.\n",
      "  'instead as a drop-in replacement.', FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.8750, device='cuda:0') 0.8750014 0.6709483\n",
      "1 tensor(0.7343, device='cuda:0') 0.734457 1.0378088\n",
      "2 tensor(0.4051, device='cuda:0') 0.40467966 0.6539176\n",
      "3 tensor(0.5527, device='cuda:0') 0.55281216 0.61515653\n",
      "4 tensor(0.4809, device='cuda:0') 0.47948065 0.6559525\n",
      "5 tensor(0.2388, device='cuda:0') 0.23760484 0.40521106\n",
      "6 tensor(0.2503, device='cuda:0') 0.24525006 0.47057593\n",
      "7 tensor(0.2891, device='cuda:0') 0.29334444 0.6718571\n",
      "8 tensor(0.3699, device='cuda:0') 0.38205132 0.77186805\n",
      "9 tensor(0.2551, device='cuda:0') 0.34926352 0.6612922\n",
      "10 tensor(0.1113, device='cuda:0') 0.11888986 0.6342341\n",
      "11 tensor(0.1729, device='cuda:0') 0.13970041 0.5351649\n",
      "12 tensor(0.2931, device='cuda:0') 0.23028748 0.50846624\n",
      "13 tensor(0.1233, device='cuda:0') 0.24191312 0.5855674\n",
      "14 tensor(0.1547, device='cuda:0') 0.07574652 0.67522186\n",
      "15 tensor(0.1507, device='cuda:0') 0.119001985 0.48846024\n",
      "16 tensor(0.1584, device='cuda:0') 0.15898554 0.6265027\n",
      "17 tensor(0.1120, device='cuda:0') 0.12637505 0.49574843\n",
      "18 tensor(0.0890, device='cuda:0') 0.13796175 0.50932556\n",
      "19 tensor(0.1110, device='cuda:0') 0.11601476 0.6227726\n",
      "20 tensor(0.0710, device='cuda:0') 0.07715966 0.59395456\n",
      "21 tensor(0.0799, device='cuda:0') 0.10700812 0.54404616\n",
      "22 tensor(0.1897, device='cuda:0') 0.13717793 0.4051016\n",
      "23 tensor(0.1814, device='cuda:0') 0.15306234 0.67971194\n",
      "24 tensor(0.0768, device='cuda:0') 0.09079688 0.50852\n",
      "25 tensor(0.0846, device='cuda:0') 0.067239724 0.58011645\n",
      "26 tensor(0.1264, device='cuda:0') 0.062463216 0.64352685\n",
      "27 tensor(0.1415, device='cuda:0') 0.09555956 0.6540568\n",
      "28 tensor(0.1835, device='cuda:0') 0.18021761 0.60812205\n",
      "29 tensor(0.0899, device='cuda:0') 0.086366765 0.70997345\n",
      "30 tensor(0.0962, device='cuda:0') 0.09214156 0.7569944\n",
      "31 tensor(0.1320, device='cuda:0') 0.10117494 0.81036985\n",
      "32 tensor(0.1506, device='cuda:0') 0.20244007 0.6702636\n",
      "33 tensor(0.0771, device='cuda:0') 0.09168593 0.5529009\n",
      "34 tensor(0.0782, device='cuda:0') 0.06337338 0.50732905\n",
      "35 tensor(0.1490, device='cuda:0') 0.11343902 0.4480098\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer.from_argparse_args(args)\n",
    "\n",
    "# `Trainer.fit`\n",
    "trainer.strategy.model = t_model\n",
    "\n",
    "# `Trainer._fit_impl`\n",
    "trainer.state.fn = TrainerFn.FITTING\n",
    "trainer.state.status = TrainerStatus.RUNNING\n",
    "trainer.training = True\n",
    "trainer._last_train_dl_reload_epoch = float(\"-inf\")\n",
    "trainer._last_val_dl_reload_epoch = float(\"-inf\")\n",
    "\n",
    "trainer._data_connector.attach_data(t_model, datamodule=dm)\n",
    "\n",
    "# `Trainer._run`\n",
    "trainer.strategy.connect(t_model)\n",
    "trainer._callback_connector._attach_model_callbacks()\n",
    "trainer._callback_connector._attach_model_logging_functions()\n",
    "\n",
    "pl.trainer.trainer.verify_loop_configurations(trainer)\n",
    "\n",
    "trainer._data_connector.prepare_data()\n",
    "trainer.strategy.setup_environment()\n",
    "trainer._call_setup_hook()\n",
    "trainer._call_configure_sharded_model()\n",
    "\n",
    "# `Trainer._run` TRAIN\n",
    "trainer.strategy.setup(trainer)\n",
    "\n",
    "# `Trainer._run_stage`\n",
    "trainer.strategy.barrier(\"run-stage\")\n",
    "trainer.strategy.dispatch(trainer)\n",
    "\n",
    "#  `Trainer._run_train`\n",
    "trainer._pre_training_routine()\n",
    "trainer.model.train()\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "trainer.fit_loop.trainer = trainer\n",
    "\n",
    "# `Trainer.fit_loop.run`\n",
    "trainer.fit_loop.reset()\n",
    "trainer.fit_loop.on_run_start()\n",
    "trainer.fit_loop.on_advance_start()\n",
    "\n",
    "# `Trainer.fit_loop.advance`\n",
    "trainer.fit_loop._data_fetcher.setup(trainer.train_dataloader, batch_to_device=functools.partial(trainer._call_strategy_hook, 'batch_to_device', dataloader_idx=0))\n",
    "\n",
    "# `Trainer.fit_loop.epoch_loop.run`\n",
    "trainer.fit_loop.epoch_loop.reset()\n",
    "trainer.fit_loop.epoch_loop.on_run_start(trainer.fit_loop._data_fetcher)\n",
    "trainer.fit_loop.epoch_loop.on_advance_start()\n",
    "\n",
    "# `Trainer.fit_loop.epoch_loop.advance`\n",
    "trainer.fit_loop.epoch_loop.val_loop.restarting = False\n",
    "trainer.fit_loop.epoch_loop.batch_progress.is_last_batch = trainer.fit_loop._data_fetcher.done\n",
    "\n",
    "data = []\n",
    "\n",
    "for batch_idx, batch in enumerate(trainer.fit_loop._data_fetcher):\n",
    "\n",
    "    kwargs = {'batch': batch, 'batch_idx': batch_idx}\n",
    "\n",
    "    trainer.fit_loop.epoch_loop.batch_progress.increment_ready()\n",
    "    trainer._logger_connector.on_batch_start(batch, batch_idx)\n",
    "\n",
    "    trainer.fit_loop.epoch_loop.batch_progress.increment_started()\n",
    "\n",
    "    # `Trainer.fit_loop.epoch_loop.batch_loop.run`\n",
    "    trainer.fit_loop.epoch_loop.batch_loop.reset()\n",
    "    trainer.fit_loop.epoch_loop.batch_loop.on_run_start(**kwargs)\n",
    "    trainer.fit_loop.epoch_loop.batch_loop.on_advance_start()\n",
    "\n",
    "\n",
    "    # `Trainer.fit_loop.epoch_loop.batch_loop.advance`\n",
    "    trainer.fit_loop.epoch_loop.batch_loop.split_idx, kwargs[\"batch\"] = trainer.fit_loop.epoch_loop.batch_loop._remaining_splits.pop(0)\n",
    "    trainer._logger_connector.on_train_split_start(trainer.fit_loop.epoch_loop.batch_loop.split_idx)\n",
    "\n",
    "    outputs = None\n",
    "\n",
    "    optimizers = pl.loops.utilities._get_active_optimizers(\n",
    "                    trainer.optimizers, trainer.optimizer_frequencies, kwargs.get(\"batch_idx\", 0)\n",
    "                )\n",
    "    # `Trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop.run`\n",
    "    trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop.reset()\n",
    "    trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop.on_run_start(kwargs['batch'], optimizers, kwargs['batch_idx'])\n",
    "    trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop.on_advance_start()\n",
    "\n",
    "    # `Trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop.advance`\n",
    "\n",
    "    # `Trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop._run_optimization`\n",
    "    split_batch = kwargs['batch']\n",
    "    batch_idx = kwargs['batch_idx']\n",
    "    optimizer = trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop._optimizers[trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop.optim_progress.optimizer_position]\n",
    "    opt_idx = trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop.optimizer_idx\n",
    "\n",
    "    # trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop._run_optimization_start(opt_idx, optimizer)\n",
    "    # closure = trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop._make_closure(split_batch, batch_idx, opt_idx, optimizer)\n",
    "\n",
    "    # print(trainer.strategy.handles_gradient_accumulation, trainer.fit_loop._should_accumulate())\n",
    "\n",
    "    # closure()\n",
    "    # result = closure.consume_result()\n",
    "\n",
    "    result = trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop._run_optimization(split_batch, batch_idx, optimizer, opt_idx)\n",
    "\n",
    "    # `Trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop.advance`\n",
    "    if result.loss is not None:\n",
    "         trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop._outputs[\n",
    "             trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop.optimizer_idx] = result.asdict()\n",
    "    trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop.optim_progress.optimizer_position += 1\n",
    "    \n",
    "    t_out = trainer.strategy.model.module.module.unet(batch.image.unsqueeze(1).cuda()).cpu().detach().numpy()\n",
    "    f_loss, f_out, f_params, f_opt_state = loop(batch, f_params, f_opt_state)\n",
    "    print(batch_idx, result.loss, f_loss, np.mean(np.abs(t_out - f_out)))\n",
    "\n",
    "# `Trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop.run`\n",
    "trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop.on_advance_end()\n",
    "trainer.fit_loop.epoch_loop.batch_loop.optimizer_loop.on_run_end()\n",
    "\n",
    "# `Trainer.fit_loop.epoch_loop.batch_loop.advance`\n",
    "trainer.fit_loop.epoch_loop.batch_loop._outputs.append(outputs)\n",
    "\n",
    "# `Trainer.fit_loop.epoch_loop.batch_loop.run`\n",
    "trainer.fit_loop.epoch_loop.batch_loop.on_advance_end()\n",
    "batch_output = trainer.fit_loop.epoch_loop.batch_loop.on_run_end()\n",
    "\n",
    "# `Trainer.fit_loop.epoch_loop.advance`\n",
    "trainer.fit_loop.epoch_loop.batch_progress.increment_processed()\n",
    "\n",
    "trainer.fit_loop.epoch_loop.update_lr_schedulers(\"step\", update_plateau_schedulers=False)\n",
    "if trainer.fit_loop.epoch_loop._num_ready_batches_reached():\n",
    "    trainer.fit_loop.epoch_loop.update_lr_schedulers(\"epoch\", update_plateau_schedulers=False)\n",
    "\n",
    "batch_end_outputs = trainer.fit_loop.epoch_loop._prepare_outputs_training_batch_end(\n",
    "    batch_output,\n",
    "    lightning_module=trainer.lightning_module,\n",
    "    num_optimizers=len(trainer.optimizers),\n",
    ")\n",
    "\n",
    "trainer._logger_connector.on_batch_end()\n",
    "trainer.fit_loop.epoch_loop.batch_progress.increment_completed()\n",
    "trainer._logger_connector.update_train_step_metrics()\n",
    "\n",
    "# `Trainer.epoch_loop.run`\n",
    "# trainer.fit_loop.epoch_loop.on_advance_end()\n",
    "# trainer.fit_loop.epoch_loop.on_run_end()\n",
    "\n",
    "# `Trainer.fit_loop.run`\n",
    "# trainer.fit_loop.on_advance_end()\n",
    "# trainer.fit_loop.on_run_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376249b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd59cd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
